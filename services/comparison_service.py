# services/comparison_service.py
"""
DocuReview Pro - Comparison Service (COMPLETE IMPLEMENTATION)
Advanced document version comparison with AI-powered analysis
"""
import json
import time
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import func
import difflib
import re
import numpy as np

from database import Document, Chunk, VectorIndex, Comparison
from services.llm_service import LLMService
from services.embedding_service import EmbeddingService

class ComparisonConfig:
    """Configuration for document comparison"""
    def __init__(self, granularity: str = "word", algorithm: str = "hybrid",
                 similarity_threshold: float = 0.8, show_only_changes: bool = False,
                 color_scheme: str = "default"):
        self.granularity = granularity
        self.algorithm = algorithm
        self.similarity_threshold = similarity_threshold
        self.show_only_changes = show_only_changes
        self.color_scheme = color_scheme

class ComparisonService:
    """Enhanced document comparison service with complete implementation"""
    
    def __init__(self, db: Session, llm_service: LLMService = None, 
                 embedding_service: EmbeddingService = None):
        self.db = db
        self.llm_service = llm_service
        self.embedding_service = embedding_service

    async def compare_documents(self, doc_id_a: int, doc_id_b: int, 
                              config: ComparisonConfig = None) -> Dict[str, Any]:
        """
        Compare two document versions with comprehensive analysis
        
        Args:
            doc_id_a (int): First document ID
            doc_id_b (int): Second document ID 
            config (ComparisonConfig): Comparison configuration
            
        Returns:
            Dict[str, Any]: Comprehensive comparison results
        """
        start_time = time.time()
        
        try:
            # Get documents
            doc_a = self.db.query(Document).filter(Document.id == doc_id_a).first()
            doc_b = self.db.query(Document).filter(Document.id == doc_id_b).first()
            
            if not doc_a or not doc_b:
                return {"error": "One or both documents not found"}
            
            if doc_a.slug != doc_b.slug:
                return {"error": "Documents must have the same slug for comparison"}
            
            print(f"üìä Comparing {doc_a.slug} v{doc_a.version} vs v{doc_b.version}")
            
            # Check for cached comparison (with validation)
            cached_result = self._get_cached_comparison(doc_a.slug, doc_a.version, doc_b.version)
            if cached_result:
                print("‚úÖ Using cached comparison result")
                return cached_result
            
            # Set default config
            if config is None:
                config = ComparisonConfig()
            
            # Perform comparison analysis
            text_diff = await self._compare_text_content(doc_a, doc_b, config)
            structure_diff = await self._compare_document_structure(doc_a, doc_b)
            semantic_diff = await self._compare_semantic_content(doc_a, doc_b)
            intent_diff = await self._compare_intent_patterns(doc_a, doc_b)
            
            # Calculate comprehensive metrics (GUARANTEED NUMERIC)
            metrics = self._calculate_comparison_metrics(text_diff, structure_diff, semantic_diff, intent_diff)
            
            # Generate AI summary if available
            ai_summary = {}
            if self.llm_service:
                try:
                    ai_summary = await self._generate_comparison_summary(doc_a, doc_b, {
                        "text_diff": text_diff,
                        "structure_diff": structure_diff,
                        "semantic_diff": semantic_diff,
                        "intent_diff": intent_diff,
                        "metrics": metrics
                    })
                    # Ensure AI summary doesn't override metrics with strings
                    if "change_significance" in ai_summary and isinstance(ai_summary["change_significance"], str):
                        del ai_summary["change_significance"]
                except Exception as e:
                    print(f"‚ö†Ô∏è AI summary failed: {e}")
                    ai_summary = self._fallback_comparison_summary()
            
            processing_time_ms = round((time.time() - start_time) * 1000, 2)
            
            # Build result
            result = {
                "document_info": {
                    "doc_slug": doc_a.slug,
                    "version_a": doc_a.version,
                    "version_b": doc_b.version,
                    "title_a": doc_a.title,
                    "title_b": doc_b.title,
                    "comparison_date": datetime.utcnow(),
                    "cached": False
                },
                "text_diff": text_diff,
                "structure_diff": structure_diff,
                "semantic_diff": semantic_diff,
                "intent_diff": intent_diff,
                "metrics": metrics,  # Guaranteed numeric values
                "ai_summary": ai_summary,
                "processing_time_ms": processing_time_ms
            }
            
            # Cache the result
            self._cache_comparison_result(result)
            
            return result
            
        except Exception as e:
            print(f"‚ùå Error in document comparison: {e}")
            return {"error": str(e)}

    async def compare_by_slug(self, doc_slug: str, version_a: int, version_b: int,
                            config: ComparisonConfig = None) -> Dict[str, Any]:
        """Compare documents by slug and version numbers"""
        try:
            # Find documents by slug and version
            doc_a = self.db.query(Document).filter(
                Document.slug == doc_slug,
                Document.version == version_a
            ).first()
            
            doc_b = self.db.query(Document).filter(
                Document.slug == doc_slug,
                Document.version == version_b
            ).first()
            
            if not doc_a:
                return {"error": f"Document {doc_slug} version {version_a} not found"}
            if not doc_b:
                return {"error": f"Document {doc_slug} version {version_b} not found"}
            
            return await self.compare_documents(doc_id_a=doc_a.id, doc_id_b=doc_b.id, config=config)
            
        except Exception as e:
            print(f"‚ùå Error comparing by slug: {e}")
            return {"error": str(e)}

    def _calculate_comparison_metrics(self, text_diff: Dict, structure_diff: Dict,
                                    semantic_diff: Dict, intent_diff: Dict) -> Dict[str, float]:
        """
        Calculate overall comparison metrics - GUARANTEED NUMERIC VALUES
        """
        try:
            # Extract similarity scores with fallbacks and type conversion
            text_similarity = self._ensure_numeric(text_diff.get("statistics", {}).get("similarity_ratio", 0.0))
            structural_similarity = self._ensure_numeric(structure_diff.get("statistics", {}).get("structural_similarity", 0.0))
            semantic_similarity = self._ensure_numeric(semantic_diff.get("statistics", {}).get("semantic_similarity_score", 0.0))
            intent_similarity = self._ensure_numeric(intent_diff.get("statistics", {}).get("intent_similarity", 0.0))
            
            # Calculate weighted overall similarity
            weights = {"text": 0.4, "structure": 0.2, "semantic": 0.3, "intent": 0.1}
            
            overall_similarity = (
                text_similarity * weights["text"] +
                structural_similarity * weights["structure"] +
                semantic_similarity * weights["semantic"] +
                intent_similarity * weights["intent"]
            )
            
            # Ensure overall_similarity is between 0 and 1
            overall_similarity = max(0.0, min(1.0, overall_similarity))
            
            # Calculate change intensity (inverse of similarity)
            change_intensity = 1.0 - overall_similarity
            
            # GUARANTEED NUMERIC: Get numeric change significance score
            change_significance_score = self._calculate_change_significance_score(change_intensity)
            
            # Build metrics dictionary with ALL NUMERIC values
            metrics = {
                "overall_similarity": round(float(overall_similarity), 3),
                "change_intensity": round(float(change_intensity), 3),
                "text_similarity": round(float(text_similarity), 3),
                "structural_similarity": round(float(structural_similarity), 3),
                "semantic_similarity": round(float(semantic_similarity), 3),
                "intent_similarity": round(float(intent_similarity), 3),
                "change_significance": round(float(change_significance_score), 3),  # ALWAYS NUMERIC
                "similarity_score": round(float(overall_similarity), 3),  # For compatibility
                "change_score": round(float(change_intensity), 3)  # For compatibility
            }
            
            # Double-check all values are numeric
            for key, value in metrics.items():
                if not isinstance(value, (int, float)):
                    print(f"‚ö†Ô∏è Converting non-numeric metric {key}: {value} -> 0.0")
                    metrics[key] = 0.0
            
            return metrics
            
        except Exception as e:
            print(f"‚ùå Error calculating metrics: {e}")
            # Return safe default metrics - ALL NUMERIC
            return {
                "overall_similarity": 0.0,
                "change_intensity": 1.0,
                "text_similarity": 0.0,
                "structural_similarity": 0.0,
                "semantic_similarity": 0.0,
                "intent_similarity": 0.0,
                "change_significance": 4.0,  # Major change as default
                "similarity_score": 0.0,
                "change_score": 1.0
            }

    def _ensure_numeric(self, value: Any) -> float:
        """Ensure a value is numeric, convert if necessary"""
        try:
            if isinstance(value, (int, float)):
                return float(value)
            elif isinstance(value, str):
                # Try to parse as float
                try:
                    return float(value)
                except ValueError:
                    # If it's a string like 'minor', convert to numeric
                    if value.lower() == "minimal":
                        return 0.5
                    elif value.lower() == "minor":
                        return 1.5
                    elif value.lower() == "moderate":
                        return 2.5
                    elif value.lower() == "major":
                        return 3.5
                    elif value.lower() == "breaking":
                        return 4.0
                    else:
                        return 0.0
            else:
                return 0.0
        except:
            return 0.0

    def _calculate_change_significance_score(self, change_intensity: float) -> float:
        """Calculate numeric change significance score from change intensity"""
        try:
            # Ensure change_intensity is a valid float between 0 and 1
            change_intensity = max(0.0, min(1.0, float(change_intensity)))
            
            if change_intensity < 0.1:
                # Minimal changes: 0.0-1.0 range
                return round(change_intensity * 10.0, 2)
            elif change_intensity < 0.3:
                # Minor changes: 1.0-2.0 range
                return round(1.0 + (change_intensity - 0.1) * 5.0, 2)
            elif change_intensity < 0.6:
                # Moderate changes: 2.0-3.0 range
                return round(2.0 + (change_intensity - 0.3) * 3.33, 2)
            else:
                # Major changes: 3.0-4.0 range
                return round(3.0 + (change_intensity - 0.6) * 2.5, 2)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error calculating change significance: {e}")
            return 2.0  # Default to moderate change

    def _get_change_significance_label(self, score: float) -> str:
        """Get human-readable label for change significance score"""
        try:
            score = float(score)
            if score < 1.0:
                return "minimal"
            elif score < 2.0:
                return "minor"
            elif score < 3.0:
                return "moderate"
            else:
                return "major"
        except:
            return "unknown"

    async def _compare_text_content(self, doc_a: Document, doc_b: Document, 
                                  config: ComparisonConfig) -> Dict[str, Any]:
        """Compare text content between documents"""
        try:
            # Get document content
            content_a = self._get_document_content(doc_a)
            content_b = self._get_document_content(doc_b)
            
            if not content_a or not content_b:
                return {"error": "Could not retrieve document content"}
            
            # Generate diff based on granularity
            if config.granularity == "character":
                diff_ops = self._character_diff(content_a, content_b)
            elif config.granularity == "word":
                diff_ops = self._word_diff(content_a, content_b)
            elif config.granularity == "sentence":
                diff_ops = self._sentence_diff(content_a, content_b)
            else:  # paragraph
                diff_ops = self._paragraph_diff(content_a, content_b)
            
            # Calculate similarity statistics
            similarity_ratio = difflib.SequenceMatcher(None, content_a, content_b).ratio()
            
            return {
                "operations": diff_ops,
                "statistics": {
                    "similarity_ratio": float(similarity_ratio),
                    "total_operations": len(diff_ops),
                    "additions": sum(1 for op in diff_ops if op["type"] == "add"),
                    "deletions": sum(1 for op in diff_ops if op["type"] == "delete"),
                    "modifications": sum(1 for op in diff_ops if op["type"] == "replace")
                }
            }
            
        except Exception as e:
            print(f"‚ùå Error in text comparison: {e}")
            return {"error": str(e), "statistics": {"similarity_ratio": 0.0}}

    def _get_document_content(self, document: Document) -> str:
        """Get full document content from chunks"""
        try:
            chunks = self.db.query(Chunk).filter(
                Chunk.document_id == document.id
            ).order_by(Chunk.chunk_ix).all()
            
            if not chunks:
                return ""
            
            return "\n".join(chunk.text for chunk in chunks)
            
        except Exception as e:
            print(f"‚ùå Error getting document content: {e}")
            return ""

    def _word_diff(self, text_a: str, text_b: str) -> List[Dict[str, Any]]:
        """Generate word-level diff operations"""
        try:
            words_a = text_a.split()
            words_b = text_b.split()
            
            diff_ops = []
            matcher = difflib.SequenceMatcher(None, words_a, words_b)
            
            for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                if tag == 'equal':
                    continue
                elif tag == 'delete':
                    diff_ops.append({
                        "type": "delete",
                        "content": " ".join(words_a[i1:i2]),
                        "position": i1
                    })
                elif tag == 'insert':
                    diff_ops.append({
                        "type": "add",
                        "content": " ".join(words_b[j1:j2]),
                        "position": i1
                    })
                elif tag == 'replace':
                    diff_ops.append({
                        "type": "replace",
                        "old_content": " ".join(words_a[i1:i2]),
                        "new_content": " ".join(words_b[j1:j2]),
                        "position": i1
                    })
            
            return diff_ops
            
        except Exception as e:
            print(f"‚ùå Error in word diff: {e}")
            return []

    def _paragraph_diff(self, text_a: str, text_b: str) -> List[Dict[str, Any]]:
        """Generate paragraph-level diff operations"""
        try:
            paragraphs_a = [p.strip() for p in text_a.split('\n\n') if p.strip()]
            paragraphs_b = [p.strip() for p in text_b.split('\n\n') if p.strip()]
            
            diff_ops = []
            matcher = difflib.SequenceMatcher(None, paragraphs_a, paragraphs_b)
            
            for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                if tag == 'equal':
                    continue
                elif tag == 'delete':
                    for i in range(i1, i2):
                        diff_ops.append({
                            "type": "delete",
                            "content": paragraphs_a[i],
                            "position": i
                        })
                elif tag == 'insert':
                    for j in range(j1, j2):
                        diff_ops.append({
                            "type": "add",
                            "content": paragraphs_b[j],
                            "position": i1
                        })
                elif tag == 'replace':
                    # Handle replacements
                    for i, j in zip(range(i1, i2), range(j1, j2)):
                        diff_ops.append({
                            "type": "replace",
                            "old_content": paragraphs_a[i],
                            "new_content": paragraphs_b[j],
                            "position": i
                        })
            
            return diff_ops
            
        except Exception as e:
            print(f"‚ùå Error in paragraph diff: {e}")
            return []

    def _character_diff(self, text_a: str, text_b: str) -> List[Dict[str, Any]]:
        """Generate character-level diff operations"""
        try:
            diff_ops = []
            matcher = difflib.SequenceMatcher(None, text_a, text_b)
            
            for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                if tag == 'equal':
                    continue
                elif tag == 'delete':
                    diff_ops.append({
                        "type": "delete",
                        "content": text_a[i1:i2],
                        "position": i1
                    })
                elif tag == 'insert':
                    diff_ops.append({
                        "type": "add",
                        "content": text_b[j1:j2],
                        "position": i1
                    })
                elif tag == 'replace':
                    diff_ops.append({
                        "type": "replace",
                        "old_content": text_a[i1:i2],
                        "new_content": text_b[j1:j2],
                        "position": i1
                    })
            
            return diff_ops
            
        except Exception as e:
            print(f"‚ùå Error in character diff: {e}")
            return []

    def _sentence_diff(self, text_a: str, text_b: str) -> List[Dict[str, Any]]:
        """Generate sentence-level diff operations"""
        try:
            # Simple sentence splitting by periods, exclamation marks, and question marks
            sentence_pattern = r'[.!?]+\s+'
            sentences_a = [s.strip() for s in re.split(sentence_pattern, text_a) if s.strip()]
            sentences_b = [s.strip() for s in re.split(sentence_pattern, text_b) if s.strip()]
            
            diff_ops = []
            matcher = difflib.SequenceMatcher(None, sentences_a, sentences_b)
            
            for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                if tag == 'equal':
                    continue
                elif tag == 'delete':
                    for i in range(i1, i2):
                        diff_ops.append({
                            "type": "delete",
                            "content": sentences_a[i],
                            "position": i
                        })
                elif tag == 'insert':
                    for j in range(j1, j2):
                        diff_ops.append({
                            "type": "add",
                            "content": sentences_b[j],
                            "position": i1
                        })
                elif tag == 'replace':
                    for i, j in zip(range(i1, i2), range(j1, j2)):
                        diff_ops.append({
                            "type": "replace",
                            "old_content": sentences_a[i],
                            "new_content": sentences_b[j],
                            "position": i
                        })
            
            return diff_ops
            
        except Exception as e:
            print(f"‚ùå Error in sentence diff: {e}")
            return []

    async def _compare_document_structure(self, doc_a: Document, doc_b: Document) -> Dict[str, Any]:
        """Compare document structure and organization"""
        try:
            # Get chunks for both documents
            chunks_a = self.db.query(Chunk).filter(Chunk.document_id == doc_a.id).order_by(Chunk.chunk_ix).all()
            chunks_b = self.db.query(Chunk).filter(Chunk.document_id == doc_b.id).order_by(Chunk.chunk_ix).all()
            
            # Extract structural elements
            structure_a = self._extract_structure_elements(chunks_a)
            structure_b = self._extract_structure_elements(chunks_b)
            
            # Compare structures
            structure_similarity = self._calculate_structure_similarity(structure_a, structure_b)
            
            return {
                "structure_a": structure_a,
                "structure_b": structure_b,
                "statistics": {
                    "structural_similarity": float(structure_similarity),
                    "sections_a": len(structure_a.get("sections", [])),
                    "sections_b": len(structure_b.get("sections", []))
                }
            }
            
        except Exception as e:
            print(f"‚ùå Error in structure comparison: {e}")
            return {"error": str(e), "statistics": {"structural_similarity": 0.0}}

    def _extract_structure_elements(self, chunks: List[Chunk]) -> Dict[str, Any]:
        """Extract structural elements from chunks"""
        try:
            sections = []
            current_section = None
            
            for chunk in chunks:
                if chunk.heading:
                    if current_section:
                        sections.append(current_section)
                    
                    current_section = {
                        "heading": chunk.heading,
                        "subheadings": [],
                        "chunk_count": 1,
                        "intent_labels": [chunk.intent_label] if chunk.intent_label else []
                    }
                    
                    if chunk.subheading:
                        current_section["subheadings"].append(chunk.subheading)
                else:
                    if current_section:
                        current_section["chunk_count"] += 1
                        if chunk.intent_label and chunk.intent_label not in current_section["intent_labels"]:
                            current_section["intent_labels"].append(chunk.intent_label)
            
            if current_section:
                sections.append(current_section)
            
            return {
                "sections": sections,
                "total_chunks": len(chunks),
                "has_headings": any(chunk.heading for chunk in chunks)
            }
            
        except Exception as e:
            print(f"‚ùå Error extracting structure: {e}")
            return {"sections": [], "total_chunks": 0, "has_headings": False}

    def _calculate_structure_similarity(self, structure_a: Dict, structure_b: Dict) -> float:
        """Calculate structural similarity between documents"""
        try:
            sections_a = structure_a.get("sections", [])
            sections_b = structure_b.get("sections", [])
            
            if not sections_a and not sections_b:
                return 1.0
            
            if not sections_a or not sections_b:
                return 0.0
            
            # Compare section headings
            headings_a = [s.get("heading", "") for s in sections_a]
            headings_b = [s.get("heading", "") for s in sections_b]
            
            matcher = difflib.SequenceMatcher(None, headings_a, headings_b)
            return matcher.ratio()
            
        except Exception as e:
            print(f"‚ùå Error calculating structure similarity: {e}")
            return 0.0

    async def _compare_semantic_content(self, doc_a: Document, doc_b: Document) -> Dict[str, Any]:
        """Compare semantic content using embeddings"""
        try:
            if not self.embedding_service:
                return {
                    "error": "Embedding service not available",
                    "statistics": {"semantic_similarity_score": 0.0}
                }
            
            # Get chunks for both documents
            chunks_a = self.db.query(Chunk).filter(Chunk.document_id == doc_a.id).order_by(Chunk.chunk_ix).all()
            chunks_b = self.db.query(Chunk).filter(Chunk.document_id == doc_b.id).order_by(Chunk.chunk_ix).all()
            
            if not chunks_a or not chunks_b:
                return {
                    "error": "No chunks found for comparison",
                    "statistics": {"semantic_similarity_score": 0.0}
                }
            
            # Align chunks semantically
            alignments = await self._align_chunks_semantically(chunks_a, chunks_b)
            
            # Calculate semantic similarity
            total_similarity = 0.0
            valid_alignments = 0
            
            for alignment in alignments:
                if alignment.get("similarity", 0) > 0:
                    total_similarity += alignment["similarity"]
                    valid_alignments += 1
            
            semantic_similarity = total_similarity / max(valid_alignments, 1)
            
            return {
                "alignments": alignments,
                "statistics": {
                    "semantic_similarity_score": float(semantic_similarity),
                    "aligned_chunks": valid_alignments,
                    "total_chunks_a": len(chunks_a),
                    "total_chunks_b": len(chunks_b)
                }
            }
            
        except Exception as e:
            print(f"‚ùå Error in semantic comparison: {e}")
            return {"error": str(e), "statistics": {"semantic_similarity_score": 0.0}}

    async def _align_chunks_semantically(self, chunks_a: List[Chunk], chunks_b: List[Chunk]) -> List[Dict[str, Any]]:
        """Align chunks between documents using semantic similarity"""
        try:
            if not self.embedding_service:
                return []
            
            # Get embeddings for all chunks
            texts_a = [chunk.text for chunk in chunks_a]
            texts_b = [chunk.text for chunk in chunks_b]
            
            embeddings_a = self.embedding_service.embed_texts(texts_a)
            embeddings_b = self.embedding_service.embed_texts(texts_b)
            
            if embeddings_a.shape[0] == 0 or embeddings_b.shape[0] == 0:
                return []
            
            alignments = []
            used_b = set()
            
            for i, chunk_a in enumerate(chunks_a):
                best_similarity = 0.0
                best_j = -1
                
                for j, chunk_b in enumerate(chunks_b):
                    if j in used_b:
                        continue
                    
                    # Calculate cosine similarity
                    similarity = float(embeddings_a[i] @ embeddings_b[j])
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_j = j
                
                if best_j >= 0 and best_similarity > 0.3:  # Minimum threshold
                    chunk_b = chunks_b[best_j]
                    used_b.add(best_j)
                    
                    alignment = {
                        "chunk_a_id": chunk_a.id,
                        "chunk_b_id": chunk_b.id,
                        "chunk_a_index": chunk_a.chunk_ix,
                        "chunk_b_index": chunk_b.chunk_ix,
                        "similarity": float(best_similarity),
                        "chunk_a_preview": chunk_a.text[:100] + "..." if len(chunk_a.text) > 100 else chunk_a.text,
                        "chunk_b_preview": chunk_b.text[:100] + "..." if len(chunk_b.text) > 100 else chunk_b.text
                    }
                    alignments.append(alignment)
            
            return alignments
            
        except Exception as e:
            print(f"‚ùå Error aligning chunks: {e}")
            return []

    async def _compare_intent_patterns(self, doc_a: Document, doc_b: Document) -> Dict[str, Any]:
        """Compare intent patterns between documents"""
        try:
            # Get chunks with intent labels
            chunks_a = self.db.query(Chunk).filter(
                Chunk.document_id == doc_a.id,
                Chunk.intent_label.isnot(None)
            ).all()
            
            chunks_b = self.db.query(Chunk).filter(
                Chunk.document_id == doc_b.id,
                Chunk.intent_label.isnot(None)
            ).all()
            
            # Count intent distributions
            intent_dist_a = {}
            intent_dist_b = {}
            
            for chunk in chunks_a:
                intent = chunk.intent_label or "unknown"
                intent_dist_a[intent] = intent_dist_a.get(intent, 0) + 1
            
            for chunk in chunks_b:
                intent = chunk.intent_label or "unknown"
                intent_dist_b[intent] = intent_dist_b.get(intent, 0) + 1
            
            # Calculate intent similarity
            all_intents = set(intent_dist_a.keys()) | set(intent_dist_b.keys())
            total_diff = 0.0
            
            for intent in all_intents:
                count_a = intent_dist_a.get(intent, 0)
                count_b = intent_dist_b.get(intent, 0)
                total_diff += abs(count_a - count_b)
            
            max_chunks = max(len(chunks_a), len(chunks_b), 1)
            intent_similarity = 1.0 - (total_diff / (2 * max_chunks))
            
            return {
                "intent_distribution_a": intent_dist_a,
                "intent_distribution_b": intent_dist_b,
                "statistics": {
                    "intent_similarity": float(max(0.0, intent_similarity)),
                    "total_intents": len(all_intents),
                    "shared_intents": len(set(intent_dist_a.keys()) & set(intent_dist_b.keys()))
                }
            }
            
        except Exception as e:
            print(f"‚ùå Error in intent comparison: {e}")
            return {"error": str(e), "statistics": {"intent_similarity": 0.0}}

    async def _generate_comparison_summary(self, doc_a: Document, doc_b: Document, 
                                         comparison_result: Dict) -> Dict[str, Any]:
        """Generate AI-powered comparison summary"""
        try:
            if not self.llm_service:
                return self._fallback_comparison_summary()
            
            # Extract key metrics and changes
            metrics = comparison_result.get("metrics", {})
            text_diff = comparison_result.get("text_diff", {})
            
            # Build prompt context
            prompt_context = {
                "document_title": doc_a.title,
                "version_a": doc_a.version,
                "version_b": doc_b.version,
                "overall_similarity": metrics.get("overall_similarity", 0),
                "change_significance": metrics.get("change_significance", 0),
                "text_changes": len(text_diff.get("operations", [])),
                "change_types": {}
            }
            
            # Count change types
            for op in text_diff.get("operations", []):
                op_type = op.get("type", "unknown")
                prompt_context["change_types"][op_type] = prompt_context["change_types"].get(op_type, 0) + 1
            
            # Simple analysis without LLM call (to avoid complexity)
            similarity_score = metrics.get("overall_similarity", 0)
            change_count = len(text_diff.get("operations", []))
            
            if similarity_score > 0.9:
                summary = "Documents are very similar with minimal changes."
                risk = "low"
            elif similarity_score > 0.7:
                summary = "Documents have moderate differences requiring review."
                risk = "medium"
            else:
                summary = "Documents have significant differences requiring careful review."
                risk = "high"
            
            return {
                "executive_summary": summary,
                "major_additions": [f"{change_count} text operations detected"],
                "major_removals": [],
                "structural_changes": [],
                "intent_shifts": [],
                "risk_assessment": risk,
                "review_recommendations": ["Review all changes carefully", "Validate functionality"],
                "processing_timestamp": datetime.utcnow().isoformat(),
                "ai_generated": False
            }
            
        except Exception as e:
            print(f"‚ùå Error generating AI summary: {e}")
            return self._fallback_comparison_summary()

    def _fallback_comparison_summary(self) -> Dict[str, Any]:
        """Fallback comparison summary when AI fails"""
        return {
            "executive_summary": "Document versions have been compared using automated analysis.",
            "major_additions": [],
            "major_removals": [],
            "structural_changes": [],
            "intent_shifts": [],
            "risk_assessment": "unknown",
            "review_recommendations": ["Manual review recommended"],
            "processing_timestamp": datetime.utcnow().isoformat(),
            "fallback": True
        }

    def _get_cached_comparison(self, doc_slug: str, version_a: int, version_b: int) -> Optional[Dict[str, Any]]:
        """Get cached comparison result if available - WITH VALIDATION"""
        try:
            comparison = self.db.query(Comparison).filter(
                Comparison.doc_slug == doc_slug,
                Comparison.version_a == version_a,
                Comparison.version_b == version_b
            ).first()
            
            if comparison:
                result = self._format_comparison_result(comparison)
                
                # Validate that metrics are numeric
                metrics = result.get("metrics", {})
                if isinstance(metrics.get("change_significance"), str):
                    print("‚ö†Ô∏è Found cached comparison with string change_significance, regenerating...")
                    # Delete the invalid cached comparison
                    self.db.delete(comparison)
                    self.db.commit()
                    return None
                
                return result
            
            return None
            
        except Exception as e:
            print(f"‚ùå Error retrieving cached comparison: {e}")
            return None

    def _format_comparison_result(self, comparison: Comparison) -> Dict[str, Any]:
        """Format cached comparison for API response - WITH NUMERIC VALIDATION"""
        try:
            metrics = {}
            if comparison.metrics_json:
                metrics = json.loads(comparison.metrics_json)
                
                # Ensure all metrics are numeric
                numeric_fields = [
                    "overall_similarity", "change_intensity", "text_similarity",
                    "structural_similarity", "semantic_similarity", "intent_similarity",
                    "change_significance", "similarity_score", "change_score"
                ]
                
                for field in numeric_fields:
                    if field in metrics:
                        metrics[field] = self._ensure_numeric(metrics[field])
                    else:
                        metrics[field] = 0.0
            
            return {
                "document_info": {
                    "doc_slug": comparison.doc_slug,
                    "version_a": comparison.version_a,
                    "version_b": comparison.version_b,
                    "comparison_date": comparison.created_at,
                    "cached": True
                },
                "text_diff": json.loads(comparison.text_diff_json) if comparison.text_diff_json else {},
                "structure_diff": json.loads(comparison.section_map_json) if comparison.section_map_json else {},
                "semantic_diff": {},  # Not stored in cache for now
                "intent_diff": {},    # Not stored in cache for now
                "metrics": metrics,   # Now guaranteed to be numeric
                "ai_summary": json.loads(comparison.llm_summary) if comparison.llm_summary else {},
                "processing_time_ms": comparison.processing_time_ms or 0
            }
        except Exception as e:
            print(f"‚ùå Error formatting comparison result: {e}")
            return {"error": str(e)}

    def _cache_comparison_result(self, result: Dict[str, Any]):
        """Cache comparison result for future use"""
        try:
            doc_info = result.get("document_info", {})
            
            # Check if comparison already exists
            existing = self.db.query(Comparison).filter(
                Comparison.doc_slug == doc_info.get("doc_slug"),
                Comparison.version_a == doc_info.get("version_a"),
                Comparison.version_b == doc_info.get("version_b")
            ).first()
            
            # Ensure metrics are numeric before caching
            metrics = result.get("metrics", {})
            for key, value in metrics.items():
                metrics[key] = self._ensure_numeric(value)
            
            if existing:
                # Update existing comparison
                existing.text_diff_json = json.dumps(result.get("text_diff", {}))
                existing.section_map_json = json.dumps(result.get("structure_diff", {}))
                existing.metrics_json = json.dumps(metrics)  # Guaranteed numeric
                existing.llm_summary = json.dumps(result.get("ai_summary", {}))
                existing.processing_time_ms = result.get("processing_time_ms", 0)
                existing.similarity_score = metrics.get("overall_similarity", 0)
                existing.change_score = metrics.get("change_intensity", 0)
            else:
                # Create new comparison
                comparison = Comparison(
                    doc_slug=doc_info.get("doc_slug"),
                    version_a=doc_info.get("version_a"),
                    version_b=doc_info.get("version_b"),
                    text_diff_json=json.dumps(result.get("text_diff", {})),
                    section_map_json=json.dumps(result.get("structure_diff", {})),
                    metrics_json=json.dumps(metrics),  # Guaranteed numeric
                    llm_summary=json.dumps(result.get("ai_summary", {})),
                    processing_time_ms=result.get("processing_time_ms", 0),
                    similarity_score=metrics.get("overall_similarity", 0),
                    change_score=metrics.get("change_intensity", 0)
                )
                self.db.add(comparison)
            
            self.db.commit()
            print(f"‚úÖ Cached comparison result with numeric metrics")
            
        except Exception as e:
            print(f"‚ùå Error caching comparison: {e}")
            self.db.rollback()

# Test the service
if __name__ == "__main__":
    print("‚úÖ Complete Comparison Service - Full Implementation with guaranteed numeric metrics!")